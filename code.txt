  1 #!/usr/bin/env python
  2
  3 import sys, os, string
  4
  5 # Utility for handling the intermediate 'secondary memory'
  6 def touchopen(filename, *args, **kwargs):
  7 try:
  8    os.remove(filename)
  9 except OSError:
 10     pass
 11 open(filename, "a").close() # "touch" file
 12 return open(filename, *args, **kwargs)
 13
 14 # The constrained memory should have no more than 1024 cells
 15 data = []
 16 # We're lucky:
 17 # The stop words are only 556 characters and the lines are all
 18 # less than 80 characters, so we can use that knowledge to
 19 # simplify the problem: we can have the stop words loaded in
 20 # memory while processing one line of the input at a time.
 21 # If these two assumptions didn't hold, the algorithm would
 22 # need to be changed considerably.
 23
 24 # Overall strategy: (PART 1) read the input file, count the
 25 # words, increment/store counts in secondary memory (a file)
 26 # (PART 2) find the 25 most frequent words in secondary memory
 27
 28 # PART 1:
 29 # - read the input file one line at a time
 30 # - filter the characters, normalize to lower case
 31 # - identify words, increment corresponding counts in file
 32
 33 # Load the list of stop words
 34 f = open('../stop_words.txt')
 35 data = [f.read(1024).split(',')] # data[0] holds the stop words
 36 f.close()
 37
 38 data.append([]) # data[1] is line (max 80 characters)
 39 data.append(None) # data[2] is index of the start_char of word
 40 data.append(0) # data[3] is index on characters, i = 0
 41 data.append(False) # data[4] is flag indicating if word was found
 42 data.append('') # data[5] is the word
 43 data.append('') # data[6] is word, NNNN
 44 data.append(0) # data[7] is frequency
 45
 46 # Open the secondary memory
 47 word_freqs = touchopen('word_freqs', 'rb+')
 48 # Open the input file
 49 f = open(sys.argv[1])
 50 # Loop over input file's lines
 51 while True:
 52 data[1] = [f.readline()]
 53 if data[1] == ['']: # end of input file
 54    break
 55 if data[1][0][len(data[1][0])-1] != '\n': # If it does not end
      with \n
 56    data[1][0] = data[1][0] + '\n' # Add \n
 57 data[2] = None
 58 data[3] = 0
 59 # Loop over characters in the line
 60 for c in data[1][0]: # elimination of symbol c is exercise
 61    if data[2] == None:
 62        if c.isalnum():
 63        # We found the start of a word
 64        data[2] = data[3]
 65    else:
 66        if not c.isalnum():
 67        # We found the end of a word. Process it
  56    data[1][0] = data[1][0] + '\n' # Add \n
 57 data[2] = None
 58 data[3] = 0
 59 # Loop over characters in the line
 60 for c in data[1][0]: # elimination of symbol c is exercise
 61    if data[2] == None:
 62        if c.isalnum():
 63        # We found the start of a word
 64        data[2] = data[3]
 65    else:
 66        if not c.isalnum():
 67        # We found the end of a word. Process it
 68        data[4] = False
 69        data[5] = data[1][0][data[2]:data[3]].lower()
 70        # Ignore words with len < 2, and stop words
 71        if len(data[5])>= 2 and data[5] not in data[0]:
 72         # Let's see if it already exists
 73         while True:
 74         data[6] = word_freqs.readline().strip()
 75         if data[6] == '':
 76            break;
 77         data[7] = int(data[6].split(',')[1])
 78         # word, no white space
 79         data[6] = data[6].split(',')[0].strip()
 80         if data[5] == data[6]:
 81            data[7] += 1
 82            data[4] = True
 83            break
 84        if not data[4]:
 85         word_freqs.seek(0, 1) # Needed in Windows
 86         word_freqs.writelines("%20s,%04d\n" % data[5], 1)
 87        else:
 88         word_freqs.seek(-26, 1)
 89         word_freqs.writelines("%20s,%04d\n" % (data[5], data[7]))
 90        word_freqs.seek(0,0)
 91        # Let's reset
 92        data[2] = None
 93     data[3] += 1
 94 # We're done with the input file
 95 f.close()
 96 word_freqs.flush()
 97
 98 # PART 2
 99 # Now we need to find the 25 most frequently occuring words.
 100 # We don't need anything from the previous values in memory
 101 del data[:]
 102
 103 # Let's use the first 25 entries for the top 25 words
 104 data = data + [[]]*(25 - len(data))
 105 data.append('') # data[25] is word,freq from file
 106 data.append(0) # data[26] is freq
 107
 108 # Loop over secondary memory file
 109 while True:
 110 data[25] = word_freqs.readline().strip()
 111 if data[25] == '': # EOF
 112    break
 113 data[26] = int(data[25].split(',')[1]) # Read it as integer
 114 data[25] = data[25].split(',')[0].strip() # word
 115 # Check if this word has more counts than the ones in memory
 116 for i in range(25): # elimination of symbol i is exercise
 117    if data[i] == [] or data[i][1] < data[26]:
 118        data.insert(i, [data[25], data[26]])
 119        del data[26] # delete the last element
 120        break
 121
 122 for tf in data[0:25]: # elimination of symbol tf is exercise
 123 if len(tf) == 2:
 124    print tf[0], ' - ', tf[1]
 125 # We're done
 126 word_freqs.close()